---
title: Architecture
description: Component design, data flow, and design rationale
author: zoobzio
published: 2026-02-21
updated: 2026-02-22
tags:
  - Architecture
  - Internals
---

# Architecture

For contributors and power users who want to understand how cldpd works internally.

## Component Overview

```
CLI (cmd/cldpd/main.go)
  |
  v
Dispatcher (dispatcher.go)
  |
  +-- Pod Discovery (pod.go)      -- filesystem -> Pod structs
  +-- Docker Runner (docker.go)   -- Pod -> built image -> running container
  |
  v
Session (session.go)              -- goroutines, pipe, event channel
  |
  v
Event channel -> caller's event loop
```

Seven source files, each with a single concern:

| File | Concern |
|------|---------|
| `errors.go` | Semantic error types |
| `event.go` | Event type constants and Event struct |
| `pod.go` | Pod discovery and configuration parsing |
| `docker.go` | Runner interface and Docker CLI implementation |
| `session.go` | Session lifecycle, goroutines, and event emission |
| `dispatcher.go` | Orchestration of the full pod lifecycle |
| `cmd/cldpd/main.go` | CLI entry point and argument parsing |

## Data Flow

### Start

```
cldpd start myrepo --issue <url>
  |
  v
Parse args: podName="myrepo", issueURL="<url>"
  |
  v
DockerRunner.Preflight() -- verify Docker is available
  |
  v
DiscoverPod("~/.cldpd/pods", "myrepo")
  |
  +-- Stat ~/.cldpd/pods/myrepo/
  +-- Stat ~/.cldpd/pods/myrepo/Dockerfile
  +-- Read ~/.cldpd/pods/myrepo/pod.json (optional)
  +-- Expand ~ in mount source paths to home directory
  +-- Read ~/.cldpd/pods/myrepo/template.md (optional)
  +-- Return Pod struct (including Template contents)
  |
  v
DockerRunner.Build(tag, pod.Dir, buildArgs)  -- synchronous, blocks
  |
  +-- docker build -t cldpd-myrepo [--build-arg K=V] ~/.cldpd/pods/myrepo/
  |
  v
newSession(sessionID, container, runner, runFn, preamble)
  |
  +-- Emit preamble: BuildStarted, BuildComplete, ContainerStarted
  +-- Create io.Pipe (pr, pw)
  +-- Spawn container goroutine: runner.Run(ctx, opts, pw) -> stores exit code, closes pw
  +-- Spawn event goroutine: bufio.Scanner(pr) -> EventOutput per line -> terminal event -> close channel
  |
  v
Return *Session to caller
  |
  v
CLI consumeSession: range over Events(), print output, wait for exit code
```

### Resume

```
cldpd resume myrepo --prompt "guidance"
  |
  v
Parse args: podName="myrepo", prompt="guidance"
  |
  v
Container name: "cldpd-myrepo" (derived from pod name)
  |
  v
newSession(sessionID, container, runner, runFn, preamble)
  |
  +-- Emit preamble: ContainerStarted
  +-- Spawn container goroutine: runner.Exec("cldpd-myrepo", cmd, pw)
  +-- Spawn event goroutine: reads lines, emits events
  |
  v
Return *Session to caller
```

### Exec Preflight

```
DockerRunner.Exec(ctx, "cldpd-myrepo", cmd, stdout)
  |
  +-- docker inspect --format '{{.State.Running}}' cldpd-myrepo
  +-- If not running -> ErrSessionNotFound
  +-- docker exec cldpd-myrepo claude --resume -p "guidance"
  +-- Blocks until command exits
```

## The Runner Interface

The `Runner` interface is the central design decision. It abstracts Docker CLI operations behind five methods:

```go
type Runner interface {
    Preflight(ctx context.Context) error
    Build(ctx context.Context, tag string, dir string, buildArgs map[string]string) error
    Run(ctx context.Context, opts RunOptions, stdout io.Writer) (int, error)
    Exec(ctx context.Context, container string, cmd []string, stdout io.Writer) (int, error)
    Stop(ctx context.Context, container string, timeout time.Duration) error
}
```

`DockerRunner` implements this interface using `os/exec`. All `exec.Cmd` construction, exit code parsing, and stderr handling are isolated here. Nothing else in the package imports `os/exec`.

Runner methods are synchronous. The async layer -- goroutines, pipes, channels -- lives in Session. This separation keeps the Runner testable with simple function-call assertions and keeps the async complexity in a single location.

This boundary serves three purposes:

1. **Testability** -- The Dispatcher can be tested with a mock Runner that never touches Docker. Unit tests cover orchestration logic; integration tests cover the Docker interaction.
2. **Substitutability** -- The interface could be implemented against a different container runtime without changing the Dispatcher or CLI.
3. **Simplicity** -- Each Runner method maps to a single Docker CLI invocation. No goroutine management, no channel coordination.

`Preflight` is part of the Runner interface rather than a DockerRunner-specific method, so that mock Runners can control preflight behaviour in tests and alternative container runtimes can implement their own availability checks.

## Session Goroutine Lifecycle

Each Session spawns exactly two goroutines:

```
newSession()
  |
  +-- goroutine 1 (container): runFn(pw) -> exitCode, exitErr (under mutex) -> pw.Close()
  |                                                                                |
  +-- goroutine 2 (event):     bufio.Scanner(pr)                                  |
                                  |                                                v
                                  +-- EventOutput per line <--- reads from pipe
                                  |
                                  +-- pr.Close() (on EOF)
                                  +-- read exitCode/exitErr from shared state
                                  +-- close(done)           <-- Wait() unblocks here
                                  +-- emit terminal event (non-blocking)
                                  +-- close(events)
```

The pipe close is the coordination mechanism. When the container goroutine finishes, it stores the exit code and error under a mutex, then closes `pw`. This causes `bufio.Scanner` in the event goroutine to reach EOF, triggering the terminal event sequence. No explicit signalling between goroutines is needed.

The `done` channel is closed *before* the terminal event is emitted, ensuring `Wait()` never blocks on event consumption. The terminal event uses a non-blocking send -- if the channel buffer is full, the event is dropped, but the channel close serves as the definitive terminal signal.

## Container Naming

Both Start and Resume use deterministic container names: `cldpd-<podName>`. This ensures Resume can always find the container created by Start for the same pod.

Session IDs remain unique (`<podName>-<hex8>`) for correlation and logging purposes, but the container name is deterministic. Docker itself is the state store -- if the container exists and is running, resume works.

## Credential Passthrough

Credentials reach the container via Docker CLI flags only. No temporary files, no disk writes:

- `inheritEnv` -- Two-tier resolution. The Dispatcher resolves each name via `os.Getenv`. Names whose values are present on the host are eagerly merged into the `Env` map; `runCmdArgs` emits `-e KEY=VALUE` flags. Names not set on the host are deferred to Docker via `InheritEnv` in `RunOptions`; `runCmdArgs` emits bare `-e NAME` flags, allowing Docker to inherit them from the host environment at run time.
- `mounts` -- `runCmdArgs` emits `-v source:target[:ro]` flags. Mount source paths starting with `~` or `~/` are expanded to the user's home directory during pod discovery, before the paths reach Docker.

## Design Q&A

**Why os/exec instead of the Docker API?**

The Docker Go SDK is a substantial dependency tree. `os/exec` wrapping the Docker CLI is a single import, and the CLI has been stable for over a decade. The tradeoff is structured responses vs. exit codes, but since cldpd does not interpret container output, exit codes are sufficient.

**Why --rm on containers?**

Pods are ephemeral. Once the task is complete and the container exits, there is no reason to keep it. `--rm` ensures cleanup. Resume works while the container is running; after exit, the container is gone and resume returns `ErrSessionNotFound`.

**Why is the Runner synchronous when the library is async?**

The Runner maps 1:1 to Docker CLI invocations. Each method blocks for a single subprocess. The async layer (goroutines, pipes, channels) lives in Session, which wraps Runner calls. This keeps Runner implementations trivially testable -- return a value, assert on arguments -- and confines concurrency complexity to one file.

**Why does Exec preflight with docker inspect?**

`docker exec` against a nonexistent container returns exit code 1, which is ambiguous -- commands also legitimately exit with code 1. `docker inspect --format '{{.State.Running}}'` provides an unambiguous check: the container exists and is running, or it does not. This is version-independent and does not rely on parsing error messages.

**Why is the Dispatcher stateless?**

The Dispatcher does not track sessions. Each `*Session` is self-contained with its own goroutines, channels, and exit state. The caller owns the handle. This eliminates session registry cleanup, leaked map entries, and concurrency guards on shared state.

**Why can events be dropped?**

The event channel has a 256-entry buffer. If the consumer falls behind, output events are dropped via `select/default` to prevent the event goroutine from blocking indefinitely. Preamble lifecycle events (`BuildStarted`, `BuildComplete`, `ContainerStarted`) block until delivered -- they are emitted synchronously before goroutines start, when the channel buffer is empty and blocking is safe.

The terminal event (`ContainerExited` or `Error`) also uses a non-blocking send. If the buffer is full when the container exits, the terminal event is dropped -- but the channel is always closed afterward, providing a definitive terminal signal. This design ensures `Wait()` never deadlocks: the `done` channel is closed before the terminal event is emitted, so `Wait()` returns regardless of event consumption.

## Performance

cldpd adds negligible overhead. The hot path is `bufio.Scanner` reading from an `io.Pipe` and sending to a buffered channel. Image builds are Docker's domain. The `docker inspect` preflight in resume adds one subprocess invocation (~50ms). Session ID generation uses `crypto/rand` for 4 bytes (~microseconds).

## Next Steps

- [API Reference](../3.reference/1.api.md) -- Function signatures and behaviour
- [Types Reference](../3.reference/2.types.md) -- Type definitions and configuration
